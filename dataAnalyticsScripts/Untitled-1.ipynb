{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark \n",
    "For PySpark, enter your Python code in the cell below. You can run the cell by hitting \"Run\" from the above toolbar. Example Python code: <br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL\n",
    "Enter %%sql in the first line and your SQL query in subsquent lines. You can run the query by hitting \"Run\" from the above toolbar. Example SQL query (you will need to replace &lt;database&gt; and &lt;table&gt; with your specific database and table): <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE DATABASE IF NOT EXISTS covid_db;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS covid_db.main_table (\n",
    "    Province_State VARCHAR(255),\n",
    "    Country_Region VARCHAR(255),\n",
    "    Lat FLOAT,\n",
    "    Long FLOAT,\n",
    "    `2020_01_22` INT,\n",
    "    `2020_01_23` INT,\n",
    "    `2020_01_24` INT,\n",
    "    `2020_01_25` INT,\n",
    "    `2020_01_26` INT,\n",
    "    `2020_01_27` INT,\n",
    "    `2020_01_28` INT,\n",
    "    `2020_01_29` INT,\n",
    "    `2020_01_30` INT,\n",
    "    `2020_01_31` INT,\n",
    "    `2020_02_01` INT\n",
    ")\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "STORED AS TEXTFILE\n",
    "LOCATION 's3://covid19-dataanalytics-mulder/data/';\n",
    "\"\"\"\n",
    "spark.sql(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"USE covid_db\")\n",
    "spark.sql(\"SHOW TABLES\").show()\n",
    "spark.sql(\"DESCRIBE TABLE main_table\").show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Se ve el esquema de la siguiente forma\n",
    "Calculation completed.\n",
    "+---------+----------+-----------+\n",
    "|namespace| tableName|isTemporary|\n",
    "+---------+----------+-----------+\n",
    "| covid_db|main_table|      false|\n",
    "+---------+----------+-----------+\n",
    "\n",
    "+--------------+------------+-------+\n",
    "|      col_name|   data_type|comment|\n",
    "+--------------+------------+-------+\n",
    "|Province_State|varchar(255)|   null|\n",
    "|Country_Region|varchar(255)|   null|\n",
    "|           Lat|       float|   null|\n",
    "|          Long|       float|   null|\n",
    "|    2020_01_22|         int|   null|\n",
    "|    2020_01_23|         int|   null|\n",
    "|    2020_01_24|         int|   null|\n",
    "|    2020_01_25|         int|   null|\n",
    "|    2020_01_26|         int|   null|\n",
    "|    2020_01_27|         int|   null|\n",
    "|    2020_01_28|         int|   null|\n",
    "|    2020_01_29|         int|   null|\n",
    "|    2020_01_30|         int|   null|\n",
    "|    2020_01_31|         int|   null|\n",
    "|    2020_02_01|         int|   null|\n",
    "+--------------+------------+-------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"SELECT * FROM main_table\")\n",
    "df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"s3a://covid19-dataanalytics-mulder/data/time_series_covid19_confirmed_global.csv\")\n",
    "df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agregar programaticamente las fechas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Generate date columns\n",
    "start_date = datetime(2020, 1, 1)\n",
    "end_date = datetime(2024, 12, 31)\n",
    "date_generated = [start_date + timedelta(days=x) for x in range((end_date - start_date).days + 1)]\n",
    "date_columns = [date.strftime(\"%Y_%m_%d\") for date in date_generated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, array, struct, lit, col\n",
    "\n",
    "\n",
    "date_columns = [col_name for col_name in date_columns if col_name in df.columns]\n",
    "\n",
    "data_cols = [struct(lit(c).alias(\"date\"), col(c).alias(\"value\")) for c in date_columns]\n",
    "\n",
    "df_long = df.select(\"Province_State\", \"Country_Region\", \"Lat\", \"Long\", explode(array(*data_cols)).alias(\"date_value\"))\n",
    "\n",
    "df_long = df_long.select(\"Province_State\", \"Country_Region\", \"Lat\", \"Long\", \"date_value.date\", \"date_value.value\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculation started (calculation_id=a2c69e51-28a4-7ae2-ae1b-b4c01bfc8df3) in (session=82c69e4f-12c9-fed7-290f-edab3b2d26cc). Checking calculation status...\n",
    "Progress:   0%|          |elapsed time = 00:00s\n",
    "Calculation a2c69e51-28a4-7ae2-ae1b-b4c01bfc8df3 failed\n",
    "\n",
    "\n",
    "  File \"<stdin>\", line 8, in <module>\n",
    "  File \"/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 1511, in select\n",
    "    jdf = self._jdf.select(self._jcols(*cols))\n",
    "  File \"/opt/amazon/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n",
    "    return_value = get_return_value(\n",
    "  File \"/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 117, in deco\n",
    "    raise converted from None\n",
    "pyspark.sql.utils.AnalysisException: cannot resolve '2020_01_01' given input columns: [spark_catalog.covid_db.main_table.2020_01_22, spark_catalog.covid_db.main_table.2020_01_23, spark_catalog.covid_db.main_table.2020_01_24, spark_catalog.covid_db.main_table.2020_01_25, spark_catalog.covid_db.main_table.2020_01_26, spark_catalog.covid_db.main_table.2020_01_27, spark_catalog.covid_db.main_table.2020_01_28, spark_catalog.covid_db.main_table.2020_01_29, spark_catalog.covid_db.main_table.2020_01_30, spark_catalog.covid_db.main_table.2020_01_31, spark_catalog.covid_db.main_table.2020_02_01, spark_catalog.covid_db.main_table.Country_Region, spark_catalog.covid_db.main_table.Lat, spark_catalog.covid_db.main_table.Long, spark_catalog.covid_db.main_table.Province_State];\n",
    "'Project [Province_State#73, Country_Region#74, Lat#75, Long#76, explode(array(struct(date, 2020_01_01, NamePlaceholder, '2020_01_01), struct(date, 2020_01_02, NamePlaceholder, '2020_01_02), struct(date, 2020_01_03, NamePlaceholder, '2020_01_03), struct(date, 2020_01_04, NamePlaceholder, '2020_01_04), struct(date, 2020_01_05, NamePlaceholder, '2020_01_05), struct(date, 2020_01_06, NamePlaceholder, '2020_01_06), struct(date, 2020_01_07, NamePlaceholder, '2020_01_07), struct(date, 2020_01_08, NamePlaceholder, '2020_01_08), struct(date, 2020_01_09, NamePlaceholder, '2020_01_09), struct(date, 2020_01_10, NamePlaceholder, '2020_01_10), struct(date, 2020_01_11, NamePlaceholder, '2020_01_11), struct(date, 2020_01_12, NamePlaceholder, '2020_01_12), struct(date, 2020_01_13, NamePlaceholder, '2020_01_13), struct(date, 2020_01_14, NamePlaceholder, '2020_01_14), struct(date, 2020_01_15, NamePlaceholder, '2020_01_15), struct(date, 2020_01_16, NamePlaceholder, '2020_01_16), struct(date, 2020_01_17, NamePlaceholder, '2020_01_17), struct(date, 2020_01_18, NamePlaceholder, '2020_01_18), struct(date, 2020_01_19, NamePlaceholder, '2020_01_19), struct(date, 2020_01_20, NamePlaceholder, '2020_01_20), struct(date, 2020_01_21, NamePlaceholder, '2020_01_21), struct(date, 2020_01_22, value, 2020_01_22#77), struct(date, 2020_01_23, value, 2020_01_23#78), struct(date, 2020_01_24, value, 2020_01_24#79), ... 1803 more fields)) AS date_value#3819]\n",
    "+- GlobalLimit 10\n",
    "   +- LocalLimit 10\n",
    "      +- Project [Province_State#73, Country_Region#74, Lat#75, Long#76, 2020_01_22#77, 2020_01_23#78, 2020_01_24#79, 2020_01_25#80, 2020_01_26#81, 2020_01_27#82, 2020_01_28#83, 2020_01_29#84, 2020_01_30#85, 2020_01_31#86, 2020_02_01#87]\n",
    "         +- SubqueryAlias spark_catalog.covid_db.main_table\n",
    "            +- HiveTableRelation [`covid_db`.`main_table`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [Province_State#73, Country_Region#74, Lat#75, Long#76, 2020_01_22#77, 2020_01_23#78, 2020_01_24#..., Partition Cols: []]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# En athena query..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT * FROM \"AwsDataCatalog\".\"covid_db\".\"main_table\" ;"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
